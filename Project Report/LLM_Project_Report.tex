\documentclass[11pt,twocolumn]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage[backend=biber]{biblatex}
\addbibresource{sources.bib}

\newcommand{\coursefootnote}{
  \renewcommand{\thefootnote}{\fnsymbol{footnote}}
  \footnotetext[1]{Course Project for DS-6051 -- Decoding Large Language Models.}
  \renewcommand{\thefootnote}{\arabic{footnote}}
}

\title{
  \textbf{Brains in Bits: A Comparative Study of PEFT Techniques for Commonsense Reasoning on GPT-2}
  
}
\author{
Vishwanath Guruvayur \\
University of Virginia \\
\texttt{vish@virginia.edu}
\and
Luke Napolitano \\
University of Virginia \\
\texttt{ljn5yms@virginia.edu}
\and
Doruk Ozar \\
University of Virginia \\
\texttt{bcp8dm@virginia.edu}
}

\date{}

\begin{document}

\maketitle
\coursefootnote   % This inserts the footnote exactly once

\begin{abstract}
Achieving Artificial General Intelligence (AGI) hinges on building models capable of flexible, human-like reasoning. A crucial component of this ability is commonsense knowledge, which can be difficult to teach a model. While industry convention has assumed that the bigger the model, the better, training large models fully is resource-intensive, often impractical, and—according to recent research—costly to the environment. In this project, we investigate whether smaller models like GPT-2 can be selectively enhanced for commonsense reasoning tasks through Parameter-Efficient Fine-Tuning (PEFT) methods, reducing the cost and impact of LLM research and use.

We initially attempted fine-tuning GPT-2 on the OpenMathInstruct-2 dataset for mathematical reasoning, but training consistently stalled with a high loss of 6.6, reflecting the model’s limitations in handling symbolic and complex instruction tasks. Recognizing these challenges, we pivoted to CommonsenseQA, a multiple-choice dataset designed to test everyday reasoning through implicit world knowledge. 

We systematically applied and compared four PEFT techniques: LoRA, Prefix Tuning, IA3, and QLoRA. Instead of updating all model parameters, these methods selectively adjust small, critical parts of the model, particularly within its attention mechanisms. Our results show that IA3 and Prefix Tuning substantially outperform LoRA and QLoRA, achieving up to 50\% validation accuracy in commonsense categories, with noticeable reductions in perplexity.

These findings suggest that PEFT methods can meaningfully strengthen specific reasoning capabilities even in smaller LLMs. Our work supports the idea that modular, targeted training strategies could form a scalable alternative to LLM training that significantly reduces the environmental and computational impact that off-the-shelf multi-billion parameter models currently offer. Future directions include expanding to ensemble PEFT methods and scaling experiments to larger models such as Mistral.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The release of ChatGPT in 2023 brought about a wave of LLM advancements that have seemed to follow a trend: bigger is better. Almost every month, a new proprietary model is released with more parameters and purportedly better performance than any model before it. However, as these top-of-the-line models have gotten more advanced, they have continued to become more expensive to train and use. 

Current approaches often involve training ever-larger models using massive datasets and computational resources. However, this brute-force strategy raises practical concerns about scalability, accessibility, and interpretability. Fine-tuning such large models from scratch is increasingly impractical, especially for smaller research groups or applications that need specialized adaptation.

However, the release of DeepSeek-R1 in January 2025 challenged this paradigm of bigger equals better and has inspired a wave of LLM research focused on the fine-tuning of LLMs and in fitting as much performance as possible in models with far fewer parameters. Driven by this research, we decided to take a deeper dive into LLM fine-tuning to better understand this technology that has quickly entered daily life.

In this project, we investigate whether smaller, more accessible models like GPT-2 can be incrementally strengthened by using parameter-efficient fine tuning (PEFT) techniques. Rather than updating millions of parameters, PEFT methods selectively adapt a small subset of components, particularly within the model's attention mechanisms, which can be thought of as the 'brain' of LLMs.

Our primary objectives are threefold:
\begin{itemize}
    \item Assess whether PEFT methods can inject common sense reasoning capabilities into a basic LLM.
    \item Compare the effectiveness of different PEFT strategies (LoRA, Prefix Tuning, IA3, and QLoRA) in improving model performance.
    \item Reflect on the broader implications of modular training approaches for LLM research and general use.
\end{itemize}

By studying how small, controlled updates can meaningfully improve commonsense reasoning, we hope to gain insights into how structured cognitive skills might be cultivated in language models without the need for massive retraining or scaling alone.



\section{Related works}
\label{sec:background}

The development of efficient fine-tuning strategies for Large Language Models (LLMs) has been an active area of research, particularly as model sizes and training costs continue to grow.

\textbf{A Critical Review of PEFT} \cite{Xu2023} covered the history of parameter-efficient fine-tuning techniques, their applications, and future direction for LLM training and tuning. The use of PEFT seems to improve parameter efficiency and reduce computational requirements.

\textbf{LoRA and QLoRA} \cite{Noble2023} explained how LoRA freezes the original weights of the model and adds a low-rank matrix that is tuned to the context. The author also briefly covered how QLoRA quantizes low-rank matrices to a lower precision.  

\textbf{Prefix Tuning} \cite{Razavi2023} summarized the addition of a learned prefix tokens to the beginning of a model's processing pipeline that help guide the model's behavior.

\textbf{IA3 (Input-Adaptive Attention)} \cite{Liu2022} extended these ideas by learning input-dependent scaling factors inside the attention and feedforward modules. IA3 showed particular promise for few-shot fine-tuning, achieving competitive results with significantly fewer trainable parameters.

Most prior work evaluated PEFT techniques on instruction-following, summarization, or dialogue tasks. However, their application to \textit{commonsense reasoning}—a core component of AGI—remains underexplored. Our project contributes by systematically comparing these PEFT methods on the CommonsenseQA benchmark, focusing on the ability to teach implicit world knowledge to a relatively small model like GPT-2.



\section{Methodology}
\label{sec:methodology}
Explain data, model(s), approach, and experimental setup.

\subsection{Data: CommonsenseQA}
Our initial experiments aimed to fine-tune GPT-2 on the OpenMathInstruct-2 dataset for mathematical reasoning. However, due to GPT-2's limited capacity to understand complex symbolic structures, training stalled at a high loss value of 6.6 without notable accuracy improvements.

Recognizing the mismatch between task complexity and model capacity, we pivoted to \textbf{CommonsenseQA}, a multiple-choice question-answering dataset focused on everyday commonsense knowledge. Each sample in CommonsenseQA consists of a question and four candidate answers, with exactly one correct choice. The dataset is designed to challenge models on reasoning about implicit world knowledge rather than relying purely on surface-level patterns, making it a suitable benchmark for our objectives.

\subsection{Model: GPT-2}
We selected the \textbf{GPT-2 small} model (approximately 125M parameters) as our base architecture. GPT-2's autoregressive nature makes it naturally effective at next-word prediction, which aligns well with the multiple-choice format where short, precise outputs are expected. Additionally, GPT-2's manageable size allowed us to conduct multiple fine-tuning experiments within reasonable compute constraints.

\subsection{Parameter Efficient Fine-Tuning Techniques}
We applied and compared four different PEFT methods:
\begin{itemize}
    \item \textbf{LoRA}: Introduces low-rank updates within attention weights.
    \item \textbf{Prefix Tuning}: Appends learned prefix tokens to each transformer layer’s input.
    \item \textbf{IA3}: Adds learnable scaling factors to the key, value, and feedforward transformations.
    \item \textbf{QLoRA}: Combines 4-bit model quantization with LoRA to enable memory-efficient fine-tuning.
\end{itemize}
Each method adapts only a small fraction of the total model parameters, keeping the core model weights frozen.

\subsection{Experimental Setup}
We trained all models using a single NVIDIA A100 GPU, leveraging the HuggingFace PEFT library for efficient implementations. Hyperparameters were held consistent across experiments for fair comparison:
\begin{itemize}
    \item Optimizer: AdamW
    \item Learning Rate: 5e-4
    \item Batch Size: 32
    \item Number of Epochs: 10
    \item Early stopping based on validation loss.
\end{itemize}

Evaluation was conducted using two primary metrics:
\begin{itemize}
    \item \textbf{Validation Accuracy}: Percentage of correct answers selected.
    \item \textbf{Perplexity}: A measure of model confidence on validation sequences.
\end{itemize}

To simulate realistic low-resource settings, we limited training to small-to-moderate compute budgets, avoiding large-scale hyperparameter searches.





\section{Experiments and Results}
\label{sec:experiments}
Present quantitative and qualitative findings. Use figures/tables if needed.

\subsection{Initial Attempts: OpenMathInstruct-2}
Our original experiments with OpenMathInstruct-2 revealed a fundamental mismatch between GPT-2’s architecture and mathematical reasoning tasks. Despite extensive training, the model’s training loss plateaued around 6.6, with negligible improvements in prediction accuracy. This indicated that the complexity and symbolic density of math datasets overwhelmed the representational limits of GPT-2.

\subsection{Pivot to CommonsenseQA}
Recognizing these challenges, we shifted to the CommonsenseQA dataset, which required shorter, sentence-level reasoning better aligned with GPT-2’s capabilities. Commonsense tasks allowed us to better assess the effectiveness of PEFT techniques without being bottlenecked by inherent architectural limitations.

\subsection{Zero-Shot Baseline}
GPT-2's zero-shot performance on CommonsenseQA was near random, achieving approximately 25\% accuracy, as expected for a four-option multiple-choice task. This baseline highlighted the necessity of fine-tuning.

\subsection{Fine-Tuning Results}
We applied four PEFT techniques and observed the following validation accuracies:

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|}
\hline
\textbf{Fine-Tuning Method} & \textbf{Validation Accuracy (\%)} \\
\hline
Zero-Shot (No Fine-Tuning) & 0.5\% \\
LoRA & 12\% \\
QLoRA & 15\% \\
Prefix Tuning & 46\% \\
IA3 & 50\% \\
\hline
\end{tabular}
\caption{Validation accuracy across different PEFT techniques on CommonsenseQA.}
\label{table:validation-accuracy}
\end{table}

\subsection{Perplexity Analysis}
In addition to accuracy, we tracked perplexity on the validation set as a measure of prediction confidence. Models fine-tuned with IA3 and Prefix Tuning showed a significant drop in perplexity compared to LoRA and QLoRA, aligning with their superior accuracy.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{perplexity_trend_placeholder.png}
\caption{Training and validation perplexity trends for different PEFT methods (placeholder figure).}
\label{fig:perplexity-trends}
\end{figure}

(Note: Actual perplexity trend plots can be generated and inserted if available.)

\subsection{Qualitative Observations}
Qualitatively, models fine-tuned with IA3 and Prefix Tuning demonstrated a stronger ability to infer implicit relationships between concepts, often selecting reasonable answers even when phrasing was ambiguous. In contrast, LoRA- and QLoRA-fine-tuned models exhibited behaviors closer to random guessing, suggesting weaker internalization of commonsense reasoning patterns.

\subsection{Summary}
Overall, our experiments indicate that IA3 and Prefix Tuning are highly effective at improving commonsense capabilities in GPT-2 under limited resource settings. LoRA and QLoRA, while efficient in other contexts, struggled to produce meaningful gains in this domain.



\section{Discussion}
\label{sec:discussion}
Interpret results, highlight limitations, analyze interesting points.


Our results show that Parameter-Efficient Fine-Tuning (PEFT) methods can meaningfully improve the commonsense reasoning capabilities of smaller language models like GPT-2, but their effectiveness varies significantly across techniques.

\subsection{Interpretation of Results}
IA3 and Prefix Tuning were substantially more effective than LoRA and QLoRA. We hypothesize that this is because IA3 and Prefix Tuning directly intervene in the model’s attention mechanisms, which are central to relational and inferential reasoning. By contrast, LoRA and QLoRA mainly inject low-rank updates into the projection layers, which may be less effective at reshaping internal reasoning processes necessary for commonsense tasks.

The significant drop in perplexity scores for IA3 and Prefix Tuning suggests that these methods help the model gain more confident and structured internal representations of everyday knowledge, even without updating most parameters.

\subsection{Limitations}
While encouraging, our experiments have important limitations:
\begin{itemize}
    \item \textbf{Model Capacity}: GPT-2 small (125M parameters) remains fundamentally limited in its ability to perform deep or multi-hop reasoning.
    \item \textbf{Dataset Simplicity}: CommonsenseQA focuses on relatively shallow commonsense tasks. More nuanced benchmarks could reveal different strengths or weaknesses.
    \item \textbf{Resource Constraints}: All training was conducted under constrained compute budgets, limiting our ability to fully optimize hyperparameters or explore longer training schedules.
\end{itemize}

\subsection{Interesting Observations}
A notable qualitative finding was that Prefix Tuning and IA3 models often selected semantically plausible answers even when wrong, suggesting partial internalization of reasoning heuristics rather than pure memorization. Meanwhile, LoRA and QLoRA-fine-tuned models frequently defaulted to random choices, implying weaker structural learning.

These insights reinforce the view that small, targeted interventions inside the model's cognitive core—its attention mechanism—can unlock better reasoning behaviors without requiring full retraining.




\section{Conclusion and Future Work}
\label{sec:conclusion}
We demonstrated that Parameter-Efficient Fine-Tuning methods significantly improve GPT-2's performance on commonsense reasoning without full model retraining. In the future, we aim to:
\begin{itemize}
    \item Explore more advanced PEFT techniques.
    \item Experiment with ensemble approaches combining multiple tuning methods.
    \item Scale experiments to larger models like Mistral.
    \item Apply these methods to more challenging and nuanced datasets.
\end{itemize}



\printbibliography[
heading=bibintoc,
title={Whole bibliography}
]

\appendix
\section{Appendix (Optional)}
Extra details, code snippets, extended tables or figures.

\end{document}
