\documentclass[11pt,twocolumn]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage[backend=biber]{biblatex}
\addbibresource{sources.bib}

\newcommand{\coursefootnote}{
  \renewcommand{\thefootnote}{\fnsymbol{footnote}}
  \footnotetext[1]{Course Project for DS-6051 -- Decoding Large Language Models.}
  \renewcommand{\thefootnote}{\arabic{footnote}}
}

\title{
  \textbf{Brains in Bits: A Comparative Study of PEFT Techniques for Commonsense Reasoning on GPT-2}
  
}
\author{
Vishwanath Guruvayur \\
University of Virginia \\
\texttt{vish@virginia.edu}
\and
Luke Napolitano \\
University of Virginia \\
\texttt{ljn5yms@virginia.edu}
\and
Doruk Ozar \\
University of Virginia \\
\texttt{bcp8dm@virginia.edu}
}

\date{}

\begin{document}

\maketitle
\coursefootnote   % This inserts the footnote exactly once

\begin{abstract}
Achieving Artificial General Intelligence (AGI) hinges on building models capable of flexible, human-like reasoning. A crucial component of this ability is commonsense knowledge, which can be difficult to teach a model. While industry convention has assumed that the bigger the model, the better, training large models fully is resource-intensive, often impractical, and—according to recent research—costly to the environment. In this project, we investigate whether smaller models like GPT-2 can be selectively enhanced for commonsense reasoning tasks through Parameter-Efficient Fine-Tuning (PEFT) methods, reducing the cost and impact of LLM research and use.

We initially attempted fine-tuning GPT-2 on the OpenMathInstruct-2 dataset for mathematical reasoning, but training consistently stalled with a high loss of 6.6, reflecting the model’s limitations in handling symbolic and complex instruction tasks. Recognizing these challenges, we pivoted to CommonsenseQA, a multiple-choice dataset designed to test everyday reasoning through implicit world knowledge. 

We systematically applied and compared four PEFT techniques: LoRA, Prefix Tuning, IA3, and QLoRA. Instead of updating all model parameters, these methods selectively adjust small, critical parts of the model, particularly within its attention mechanisms. Our results show that IA3 and Prefix Tuning substantially outperform LoRA and QLoRA, achieving up to 50\% validation accuracy in commonsense categories, with noticeable reductions in perplexity.

These findings suggest that PEFT methods can meaningfully strengthen specific reasoning capabilities even in smaller LLMs. Our work supports the idea that modular, targeted training strategies could form a scalable alternative to LLM training that significantly reduces the environmental and computational impact that off-the-shelf multi-billion parameter models currently offer. Future directions include expanding to ensemble PEFT methods and scaling experiments to larger models such as Mistral.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\indent The release of ChatGPT in late 2022 brought about a wave of LLM advancements that have seemed to follow a trend: bigger is better. Almost every month, a new proprietary model is released with more parameters and purportedly better performance than any model before it. However, as these top-of-the-line models have gotten more advanced, they have continued to become more expensive to train and use. 

Current approaches often involve training ever-larger models using massive datasets and computational resources. However, this brute-force strategy raises practical concerns about scalability, accessibility, and interpretability. Fine-tuning such large models from scratch is increasingly impractical, especially for smaller research groups or applications that need specialized adaptation.

However, the release of DeepSeek-R1 in January 2025 challenged this paradigm of bigger equals better and has inspired a wave of LLM research focused on the fine-tuning of LLMs and in fitting as much performance as possible in models with far fewer parameters. Driven by this research, we decided to take a deeper dive into LLM fine-tuning to better understand this technology that has quickly entered daily life.

In this project, we investigate whether smaller, more accessible models like GPT-2 can be incrementally strengthened by using parameter-efficient fine tuning (PEFT) techniques. Rather than updating millions of parameters, PEFT methods selectively adapt a small subset of components, particularly within the model's attention mechanisms, which can be thought of as the 'brain' of LLMs.

Our primary objectives are threefold:
\begin{itemize}
    \item Assess whether PEFT methods can inject common sense reasoning capabilities into a basic LLM.
    \item Compare the effectiveness of different PEFT strategies (LoRA, Prefix Tuning, IA3, and QLoRA) in improving model performance.
    \item Understand how different types of reparametrization and adaptive techniques affect different cognitive abilities of the LLM Transformer Architecture.
    \item Reflect on the broader implications of modular training approaches for LLM research and general use.
\end{itemize}

By studying how small, controlled updates can meaningfully improve commonsense reasoning, we hope to gain insights into how structured cognitive skills might be cultivated in language models without the need for massive retraining or scaling alone.



\section{Related works}
\label{sec:background}

The development of efficient fine-tuning strategies for Large Language Models (LLMs) has been an active area of research, particularly as model sizes and training costs continue to grow.

\textbf{A Critical Review of PEFT} \cite{Xu2023} covered the history of parameter-efficient fine-tuning techniques, their applications, and future direction for LLM training and tuning. The use of PEFT seems to improve parameter efficiency and reduce computational requirements.

\textbf{LoRA and QLoRA} \cite{Noble2023} explained how LoRA freezes the original weights of the model and adds a low-rank matrix that is tuned to the context. The author also briefly covered how QLoRA quantizes low-rank matrices to a lower precision.  

\textbf{Prefix Tuning} \cite{Razavi2023} summarized the addition of a learned prefix tokens to the beginning of a model's processing pipeline that help guide the model's behavior.

\textbf{IA3 (Input-Adaptive Attention)} \cite{Liu2022} extended these ideas by learning input-dependent scaling factors inside the attention and feedforward modules. IA3 showed particular promise for few-shot fine-tuning, achieving competitive results with significantly fewer trainable parameters.

Most prior work evaluated PEFT techniques on instruction-following, summarization, or dialogue tasks. However, their application to \textit{commonsense reasoning} which is a core component of AGI remains rather underexplored. Our project contributes by systematically comparing these PEFT methods on the CommonsenseQA benchmark, focusing on the ability to teach implicit world knowledge to a relatively small model like GPT-2.



\section{Methodology}
\label{sec:methodology}
In this section we will discuss our approach to experiment with different PEFT methods for our finalized Dataset.

\subsection{Data: CommonsenseQA}
Our initial experiments aimed to fine-tune GPT-2 on the OpenMathInstruct-2 dataset for mathematical reasoning. However, due to GPT-2's limited capacity to understand complex symbolic structures, training stalled at a high loss value of 6.6 without notable accuracy improvements.

We did not want to move to a larger model like Mistral 6B Instruct Model because we wanted to test the capabilities of PEFT methods in a very basic GPT like GPT-2 and not have any beneficiary aspects of having a larger model.

Recognizing the mismatch between task complexity and model capacity, we pivoted to \textbf{CommonsenseQA}, a multiple-choice question-answering dataset focused on everyday commonsense knowledge. Each sample in CommonsenseQA consists of a question and five candidate answers, with exactly one correct choice. The dataset is designed to challenge models on reasoning about implicit world knowledge rather than relying purely on surface-level patterns, making it a suitable benchmark for our objectives.

The dataset also has 785 categories of these common sense questions. We clustered these categories into a larger set of 10 broad classes for a better grasp of the classes.

\subsection{Model: GPT-2 with PEFTs}
We selected the \textbf{GPT-2 small} model (approximately 125M parameters) as our base architecture. GPT-2's autoregressive nature makes it naturally effective at next-word prediction, which aligns well with the multiple-choice format where short, precise outputs are expected. Additionally, GPT-2's manageable size allowed us to conduct multiple fine-tuning experiments within reasonable compute constraints.

\subsection{Parameter Efficient Fine-Tuning Techniques}
We applied and compared four different PEFT methods:
\begin{itemize}
    \item \textbf{LoRA}: Introduces low-rank updates within attention weights.
    \item \textbf{Prefix Tuning}: Appends learned prefix tokens to each transformer layer’s input.
    \item \textbf{IA3}: Adds learnable scaling factors to the key, value, and feedforward transformations.
    \item \textbf{QLoRA}: Combines 4-bit model quantization with LoRA to enable memory-efficient fine-tuning.
\end{itemize}
Each method adapts only a small fraction of the total model parameters, keeping the core model weights frozen.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../graphs/PEFTs.png}
\caption{Intuitive Visualization of the different target regions for each PEFT Method}
\label{fig:peft-target}
\end{figure}

\subsection{Experimental Setup}
We trained all models using a single NVIDIA A100 GPU, leveraging the HuggingFace PEFT library for efficient implementations. Hyperparameters were held consistent across experiments for fair comparison:
\begin{itemize}
    \item Optimizer: AdamW
    \item Learning Rate: 5e-4
    \item Batch Size: 32
    \item Number of Epochs: 10
    \item Early stopping based on validation loss.
\end{itemize}

Evaluation was conducted using two primary metrics:
\begin{itemize}
    \item \textbf{Validation Accuracy}: Percentage of correct answers selected. We also calculated this accuracy within the broad classes of questions to understand each method's expertise in specific types of common sense.
    \item \textbf{Perplexity}: A measure of model confidence on validation sequences.
\end{itemize}

To simulate realistic low-resource settings, we limited training to small-to-moderate compute budgets, avoiding large-scale hyperparameter searches.


\section{Experiments and Results}
\label{sec:experiments}

\subsection{Initial Exploration: OpenMathInstruct-2}
Our early experiments on the OpenMathInstruct-2 dataset exposed a fundamental mismatch between GPT-2’s architecture and the demands of mathematical reasoning. Despite extensive training, the model's loss plateaued at approximately 6.6, with minimal improvements in prediction accuracy. This stagnation suggested that the symbolic complexity and structured reasoning required by math datasets exceeded GPT-2’s representational capacity.

\subsection{Strategic Pivot: CommonsenseQA}
Recognizing these limitations, we transitioned to the CommonsenseQA dataset, which emphasizes sentence-level, everyday reasoning—better suited to GPT-2's strengths. This shift allowed us to more accurately assess the effectiveness of parameter-efficient fine-tuning (PEFT) methods without architectural bottlenecks dominating outcomes.

\subsection{Zero-Shot Baseline Performance}
Without fine-tuning, GPT-2’s zero-shot accuracy on CommonsenseQA was close to random, at roughly 6.35\%—consistent with a four-choice multiple-selection task. This established a clear need for targeted fine-tuning.

\subsection{Fine-Tuning Outcomes}
We applied four PEFT strategies and recorded the following validation accuracies:

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|}
\hline
\textbf{Method} & \textbf{Validation Accuracy (\%)} \\
\hline
Zero-Shot (1.5b)& 6.35\%\\
LoRA & 12\% \\
QLoRA & 15\% \\
Prefix Tuning & 46\% \\
IA3 & 50\% \\
\hline
\end{tabular}
\caption{Validation accuracy across different PEFT techniques on CommonsenseQA.}
\label{table:validation-accuracy}
\end{table}

Prefix Tuning and IA3 clearly outperformed LoRA and QLoRA, achieving near 50\% accuracy and all 4 PEFT methods outperformed the largest available GPT-2 model.

\subsection{Training and Validation Loss Dynamics}
We monitored the training and validation losses to better understand convergence behaviors for each PEFT method.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../graphs/LoRA.png}
\caption{Training and validation loss curves for LoRA fine-tuning.}
\label{fig:lora-loss}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../graphs/QLoRA.png}
\caption{Training and validation loss curves for QLoRA fine-tuning.}
\label{fig:qlora-loss}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../graphs/Prefix.png}
\caption{Training and validation loss curves for Prefix Tuning fine-tuning.}
\label{fig:prefix-loss}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../graphs/IA3.png}
\caption{Training and validation loss curves for IA3 fine-tuning.}
\label{fig:ia3-loss}
\end{figure}

Consistently, IA3 and Prefix Tuning converged faster and to lower final validation losses compared to LoRA and QLoRA, mirroring their superior accuracies.

\subsection{Perplexity Analysis}

Although CommonsenseQA is a classification task, we also measured perplexity after fine-tuning to assess general model calibration. Table~\ref{table:perplexity-accuracy} summarizes the results.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|}
\hline
\textbf{Method} & \textbf{Perplexity}\\
\hline
Zero Shot & 5536.87\\
LoRA & 14.31 \\
QLoRA & 14.45\\
Prefix Tuning & 14.06\\
IA3 & 16.24\\
\hline
\end{tabular}
\caption{Perplexity and validation accuracy across PEFT methods on CommonsenseQA.}
\label{table:perplexity-accuracy}
\end{table}

Interestingly, perplexity values across PEFT methods were relatively close and did not correlate strongly with classification accuracy. For instance, IA3 achieved higher accuracy despite having a higher perplexity than LoRA and QLoRA. This suggests that perplexity alone is not a reliable indicator of multiple-choice reasoning performance in this setting.

\subsection{Category-Wise Analysis}
To further probe model behavior, we evaluated category-specific performance across commonsense reasoning types within CommonsenseQA.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../graphs/Categorical_Performance.png}
\caption{Validation accuracy by commonsense category across PEFT methods.}
\label{fig:category-accuracy}
\end{figure}

IA3 and Prefix Tuning consistently led across multiple categories, with especially strong gains in social and temporal reasoning.

\subsection{Qualitative Assessment}
Qualitatively, IA3- and Prefix-fine-tuned models showed a stronger grasp of implicit relationships between concepts, often selecting plausible answers even when questions were phrased ambiguously. In contrast, LoRA- and QLoRA-fine-tuned models behaved closer to random guessing, suggesting weaker internalization of commonsense structures.

\subsection{Summary of Findings}
Overall, our experiments demonstrate that \textbf{IA3 and Prefix Tuning are highly effective} for enhancing commonsense reasoning in GPT-2 under resource constraints. While \textbf{LoRA and QLoRA} remain efficient in other contexts, they struggled to deliver meaningful improvements on CommonsenseQA.




\section{Discussion}
\label{sec:discussion}
Our results show that Parameter-Efficient Fine-Tuning (PEFT) methods can significantly improve the commonsense reasoning capabilities of smaller language models like GPT-2, but their effectiveness varies significantly across techniques.

\subsection{Interpretation of Results}
IA3 and Prefix Tuning were substantially more effective than LoRA and QLoRA. We hypothesize that this is because IA3 and Prefix Tuning directly intervene in the model’s attention mechanisms, which are central to relational and inferential reasoning. By contrast, LoRA and QLoRA mainly inject low-rank updates into the projection layers, which may be less effective at reshaping internal reasoning processes necessary for commonsense tasks.

The significant drop in perplexity scores for IA3 and Prefix Tuning suggests that these methods help the model gain more confident and structured internal representations of everyday knowledge, even without updating most parameters.

\subsection{Limitations}
While encouraging, our experiments have important limitations:
\begin{itemize}
    \item \textbf{Model Capacity}: GPT-2 small (125M parameters) remains fundamentally limited in its ability to perform deep or multi-hop reasoning.
    \item \textbf{Dataset Simplicity}: CommonsenseQA focuses on relatively shallow commonsense tasks. More nuanced benchmarks could reveal different strengths or weaknesses.
    \item \textbf{Resource Constraints}: All training was conducted under constrained compute budgets, limiting our ability to fully optimize hyperparameters or explore longer training schedules.
\end{itemize}

\subsection{Interesting Observations}
A notable qualitative finding was that Prefix Tuning and IA3 models often selected semantically plausible answers even when wrong, suggesting partial internalization of reasoning heuristics rather than pure memorization. Meanwhile, LoRA and QLoRA-fine-tuned models frequently defaulted to random choices, implying weaker structural learning.

These insights reinforce the view that small, targeted interventions inside the model's cognitive core—its attention mechanism—can unlock better reasoning behaviors without requiring full retraining.




\section{Conclusion and Future Work}
\label{sec:conclusion}
We demonstrated that Parameter-Efficient Fine-Tuning methods significantly improve GPT-2's performance on commonsense reasoning without full model retraining. In the future, we aim to:
\begin{itemize}
    \item Explore more advanced PEFT techniques.
    \item Experiment with ensemble approaches combining multiple tuning methods.
    \item Scale experiments to larger models like Mistral.
    \item Apply these methods to more challenging and nuanced datasets.
\end{itemize}



\printbibliography[
heading=bibintoc,
title={Whole bibliography}
]

\appendix
\section{Appendix (Optional)}
Extra details, code snippets, extended tables or figures.

\end{document}
