@misc{Xu2023,
      title={Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment}, 
      author={Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},
      year={2023},
      eprint={2312.12148},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.12148}, 
}
@misc{Patterson2022,
      title={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink}, 
      author={David Patterson and Joseph Gonzalez and Urs HÃ¶lzle and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
      year={2022},
      eprint={2204.05149},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2204.05149}, 
}
@misc{Noble2023,
      title={What is LoRA?}, 
      author={Joshua Noble},
      year={2025},
      primaryClass={cs.CL},
      url={https://www.ibm.com/think/topics/lora}, 
}

@misc{Razavi2023,
      title={Understanding Prefix Tuning}, 
      author={Ali Razavi},
      year={2023},
      primaryClass={cs.CL},
      url={https://medium.com/@razavipour6/understanding-prefix-tuning-a-novel-approach-to-fine-tuning-language-models-dc7dafeb32e4}, 
}

@misc{Liu2022,
      title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning}, 
      author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
      year={2022},
      eprint={2205.05638},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.05638}, 
}